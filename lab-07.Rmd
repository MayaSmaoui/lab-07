---
title: "Lab 07 - Modelling course evaluations"
author: "Maya Smaoui"
date: "`r Sys.Date()`"
output: html_document
---

### Packages and Data

```{r load-packages, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)

```


```{r read-data}
evals<-read.csv("data/evals.csv", row.names=1)
```


# Exercise 1: Exploratory Data Analysis

1.  Visualize the distribution of `score` in the dataframe `evals`.

```{r viz-score}

ggplot(evals, aes(x = score)) +
  geom_bar()

evals_stats <- evals %>%
  summarise(median_score = median(score), mean_score = mean(score))

evals_stats
```

*The distribution is left skewed. It tells us that the majority of students rate courses with high scores. Yes, I expected to see more high scores than low scores, because students are usually satisfied of their courses. 

The median score is 4.3 / 5 ( which is quite high ) and the average score is about 4.17 / 5*

2.  Visualize and describe the relationship between `score` and `bty_avg` using `geom_point()` to represent the data. 

```{r scatterplot}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter()
```

*Jitter gives a better view of overlapping data points. What was misleading in the first plot is that some points overlapped but we couldn't see them, thus the plot wasn't a right representation of the overlapping data.*

# Exercise 2: Simple Linear regression with a numerical predictor

1. Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` from average beauty rating (`bty_avg`). Print the regression output using `tidy()`.

```{r fit-score_bty_fit, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals)
```

```{r tidy-score_bty_fit, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
tidy(score_bty_fit)
```

*score_hat = 3.88 + 0.067*bty_avg *

2. Plot the data again using `geom_jitter()`, and add the regression line.

```{r viz-score_bty_fit,eval=FALSE}
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE)
```

3. Interpret the slope of the linear model in context of the data.

*For each additional point in beauty average, we would expect the score to be higher, on average by 0.067 out of 5.*

4. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.

*Teachers with 0 point in beauty_average are expected to have a score of 3.88 out of 5 on average. The intercept does make sense in this context.*

5. Determine the $R^2$ of the model and interpret it in the context of the data.

```{r R2, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(score_bty_fit)$r.squared
```

*The percentage of variability in the score explained by the regression model is 0.035.*

6. Make a plot of residuals vs. predicted values for the model above.

```{r viz-score_bty_fit-diagnostic, eval = FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
score_bty_aug <- augment(score_bty_fit$fit)

ggplot(score_bty_aug, aes(x = .fitted, y = .resid)) +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```

# Exercise 3: Simple Linear regression with a categorical predictor

0. Look at the variable rank, and determine the frequency of each category level.

```{r}
# ... 
```

1. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.

```{r fit-score_rank_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

2. Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor. 

```{r fit-score_gender_fit}
# fit model

# tidy model output
```

```{r score_gender_intercept, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_intercept <- tidy(score_gender_fit) %>% 
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  pull()
```

```{r score_gender_slope, eval=FALSE}
# remove eval = FALSE from the code chunk options
score_gender_slope <- tidy(score_gender_fit) %>% 
  filter(term == "gendermale") %>%
  select(estimate) %>%
  pull()
```

*Add your narrative here. Use in-line code!*

# Exercise 4: Multiple linear regression

1. Fit a multiple linear regression model, predicting average professor evaluation `score` based on average beauty rating (`bty_avg`) and `gender.`

```{r fit-score_bty_gender_fit}
# fit model

# tidy model output
```

*Add your narrative here.*

```{r eval = FALSE}
ggplot(___) + ...
```

2. What percent of the variability in `score` is explained by the model `score_bty_gender_fit`. 

```{r}
# ...
```


3. What is the equation of the line corresponding to just male professors?

*Add your equation here.*

4. For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

*Add your narrative here.*

5. How does the relationship between beauty and evaluation score vary between male and female professors?

*Add your narrative here.*

6. How do the adjusted $R^2$ values of `score_bty_fit` and `score_bty_gender_fit` compare? 

```{r eval=FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(___)$adj.r.squared
glance(___)$adj.r.squared
```

*Add your narrative here.*

7. Compare the slopes of `bty_avg` under the two models (`score_bty_fit` and `score_bty_gender_fit`).

*Add your narrative here.*

# Exercise 5: Interpretation of log-transformed response variables

If you do not know how to use LaTeX, do this exercise with pen and paper.
